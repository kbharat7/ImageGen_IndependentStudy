{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "from torchvision.datasets.folder import ImageFolder\n",
        "from torchvision.datasets.folder import default_loader\n",
        "import timm\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1713551186757
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "img_height, img_width = 512, 512\n",
        "batch_size = 16\n",
        "\n",
        "# Data Loaders without normalization to calculate mean and std\n",
        "calc_transforms = transforms.Compose([\n",
        "    transforms.Resize((img_height, img_width)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def get_mean_and_std(loader):\n",
        "    # Var[X] = E[X^2] - (E[X])^2\n",
        "    # So we will first calculate E[X] and E[X^2] to determine mean and std dev\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        # Shape of data is [batch_size, 3, height, width]\n",
        "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
        "        num_batches += 1\n",
        "\n",
        "    mean = channels_sum / num_batches\n",
        "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
        "    return mean, std\n",
        "\n",
        "# Calculate mean and std for dataset\n",
        "train_data_for_calc = datasets.ImageFolder(root='Train_Original/Train/Train', transform=calc_transforms)\n",
        "train_loader_for_calc = DataLoader(train_data_for_calc, batch_size=16, shuffle=True, num_workers=8, pin_memory=True)\n",
        "mean, std = get_mean_and_std(train_loader_for_calc)\n",
        "print(f'Calculated Mean: {mean}, Calculated Std: {std}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Calculated Mean: tensor([0.4966, 0.4965, 0.4964]), Calculated Std: tensor([0.2105, 0.2105, 0.2106])\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551207147
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up data transformations and custom dataset class\n",
        "def get_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4965, 0.4965, 0.4964],\n",
        "                             std=[0.2105, 0.2105, 0.2106])\n",
        "    ])"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551223696
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(datasets.ImageFolder):\n",
        "    def __getitem__(self, index):\n",
        "        path, target = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "        sample = sample.convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, target"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551225809
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and DataLoader setup\n",
        "def get_dataloaders(train_dir, val_dir, batch_size):\n",
        "    train_transforms = get_transforms()\n",
        "    val_transforms = get_transforms()\n",
        "\n",
        "    train_dataset = CustomDataset(root=train_dir, transform=train_transforms)\n",
        "    val_dataset = CustomDataset(root=val_dir, transform=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, train_dataset"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551230149
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(model, device, num_classes):\n",
        "    # Load the ResNet-50 model pre-trained on ImageNet from timm\n",
        "    model = timm.create_model(model, pretrained=True, num_classes=num_classes)\n",
        "    \n",
        "    # No need to modify the final layer manually as in torchvision, \n",
        "    # timm handles this with the num_classes parameter in create_model\n",
        "    model = model.to(device)\n",
        "    \n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551233792
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(train_dataset):\n",
        "    # Count the number of occurrences of each class\n",
        "    class_counts = torch.zeros((len(train_dataset.classes)), dtype=torch.float)\n",
        "    for _, label in train_dataset:\n",
        "        class_counts[label] += 1\n",
        "    \n",
        "    # Compute class weights (inverse frequency or another method)\n",
        "    class_weights = 1. / class_counts\n",
        "    class_weights = (class_weights / class_weights.sum()) * len(train_dataset.classes)\n",
        "    return class_weights"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551238558
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation function\n",
        "def validate_model(model, val_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = val_loss / len(val_loader.dataset)\n",
        "    epoch_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "    return epoch_loss, epoch_acc"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551244770
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with mixed precision\n",
        "def train_model(model_name, model, train_loader, device, epochs, class_weights):\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.01)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        val_loss, val_acc = validate_model(model, val_loader, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Save the best model based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "        \n",
        "    # Save the best model found during training\n",
        "    if best_model_state:\n",
        "        torch.save(best_model_state, f'{model_name}_final.pth')\n",
        "        print(\"Saved the best model based on validation loss.\")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713551249378
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {torch.cuda.get_device_name(0)}')\n",
        "\n",
        "val_dir = 'Valid/'\n",
        "train_dir = 'Cleaned_Images/'\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "epochs = 100\n",
        "\n",
        "train_loader, val_loader, train_dataset = get_dataloaders(train_dir, val_dir, batch_size)\n",
        "\n",
        "class_weights = calculate_class_weights(train_dataset)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "model_resnet = setup_model('resnet50', device, num_classes)\n",
        "# model_efficientnet = setup_model('tf_efficientnet_b7', device, num_classes)\n",
        "train_model('resnet50', model_resnet, train_loader, device, epochs, class_weights)\n",
        "# train_model('tf_efficientnet_b7', model_efficientnet, train_loader, device, epochs, class_weights)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using device: NVIDIA A100 80GB PCIe\nEpoch 1/100, Train Loss: 0.8850, Val Loss: 0.8710, Val Acc: 0.6150\nEpoch 2/100, Train Loss: 0.4257, Val Loss: 0.7312, Val Acc: 0.5800\nEpoch 3/100, Train Loss: 0.2303, Val Loss: 0.7763, Val Acc: 0.6200\nEpoch 4/100, Train Loss: 0.1720, Val Loss: 0.7440, Val Acc: 0.6350\nEpoch 5/100, Train Loss: 0.1479, Val Loss: 0.7792, Val Acc: 0.6350\nEpoch 6/100, Train Loss: 0.1245, Val Loss: 0.7412, Val Acc: 0.6500\nEpoch 7/100, Train Loss: 0.1109, Val Loss: 0.7403, Val Acc: 0.6700\nEpoch 8/100, Train Loss: 0.0972, Val Loss: 0.7263, Val Acc: 0.6600\nEpoch 9/100, Train Loss: 0.0830, Val Loss: 0.7802, Val Acc: 0.6550\nEpoch 10/100, Train Loss: 0.0699, Val Loss: 0.8230, Val Acc: 0.6700\nEpoch 11/100, Train Loss: 0.0513, Val Loss: 0.9345, Val Acc: 0.6650\nEpoch 12/100, Train Loss: 0.0481, Val Loss: 1.0397, Val Acc: 0.6600\nEpoch 13/100, Train Loss: 0.0379, Val Loss: 1.0415, Val Acc: 0.6450\nEpoch 14/100, Train Loss: 0.0347, Val Loss: 1.0412, Val Acc: 0.6750\nEpoch 15/100, Train Loss: 0.0243, Val Loss: 1.1123, Val Acc: 0.6700\nEpoch 16/100, Train Loss: 0.0230, Val Loss: 1.2605, Val Acc: 0.6450\nEpoch 17/100, Train Loss: 0.0231, Val Loss: 1.2259, Val Acc: 0.6600\nEpoch 18/100, Train Loss: 0.0235, Val Loss: 1.2929, Val Acc: 0.6600\nEpoch 19/100, Train Loss: 0.0202, Val Loss: 1.2301, Val Acc: 0.6450\nEpoch 20/100, Train Loss: 0.0190, Val Loss: 1.2018, Val Acc: 0.6600\nEpoch 21/100, Train Loss: 0.0185, Val Loss: 1.2176, Val Acc: 0.6450\nEpoch 22/100, Train Loss: 0.0201, Val Loss: 1.1697, Val Acc: 0.6750\nEpoch 23/100, Train Loss: 0.0220, Val Loss: 1.1490, Val Acc: 0.6500\nEpoch 24/100, Train Loss: 0.0208, Val Loss: 1.2138, Val Acc: 0.6650\nEpoch 25/100, Train Loss: 0.0199, Val Loss: 1.1216, Val Acc: 0.6800\nEpoch 26/100, Train Loss: 0.0189, Val Loss: 1.1777, Val Acc: 0.6500\nEpoch 27/100, Train Loss: 0.0181, Val Loss: 1.1896, Val Acc: 0.6750\nEpoch 28/100, Train Loss: 0.0204, Val Loss: 1.1126, Val Acc: 0.7000\nEpoch 29/100, Train Loss: 0.0184, Val Loss: 1.2033, Val Acc: 0.6700\nEpoch 30/100, Train Loss: 0.0184, Val Loss: 1.1702, Val Acc: 0.6600\nEpoch 31/100, Train Loss: 0.0175, Val Loss: 1.1642, Val Acc: 0.6900\nEpoch 32/100, Train Loss: 0.0190, Val Loss: 1.1319, Val Acc: 0.6900\nEpoch 33/100, Train Loss: 0.0208, Val Loss: 1.1515, Val Acc: 0.6900\nEpoch 34/100, Train Loss: 0.0180, Val Loss: 1.1194, Val Acc: 0.6850\nEpoch 35/100, Train Loss: 0.0203, Val Loss: 1.2544, Val Acc: 0.6700\nEpoch 36/100, Train Loss: 0.0193, Val Loss: 1.1117, Val Acc: 0.6600\nEpoch 37/100, Train Loss: 0.0195, Val Loss: 1.1378, Val Acc: 0.6950\nEpoch 38/100, Train Loss: 0.0192, Val Loss: 1.1767, Val Acc: 0.6700\nEpoch 39/100, Train Loss: 0.0192, Val Loss: 1.0877, Val Acc: 0.6800\nEpoch 40/100, Train Loss: 0.0188, Val Loss: 1.1590, Val Acc: 0.6650\nEpoch 41/100, Train Loss: 0.0175, Val Loss: 1.1461, Val Acc: 0.6750\nEpoch 42/100, Train Loss: 0.0189, Val Loss: 1.1709, Val Acc: 0.6800\nEpoch 43/100, Train Loss: 0.0199, Val Loss: 1.1328, Val Acc: 0.6800\nEpoch 44/100, Train Loss: 0.0199, Val Loss: 1.1564, Val Acc: 0.6800\nEpoch 45/100, Train Loss: 0.0201, Val Loss: 1.1333, Val Acc: 0.6900\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_84468/2429079452.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713548073209
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()  # Or use .get() with explicit parameters\n",
        "compute_target = ws.compute_targets['akrishn21']\n",
        "compute_target.stop(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1713548073355
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "myenv",
      "language": "python",
      "display_name": "Python (myenv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "myenv"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}